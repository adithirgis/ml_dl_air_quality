---
title: "DRF"
author: "Adithi R. Upadhya"
date: "30/11/2021"
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r warning = FALSE}
library(xgboost)
library(readxl)
library(tidyverse)
library(tidymodels)
library(h2o)
library(here)
library(data.table)

set.seed(108)

file_shared <- read_excel(here("data", "Final_Delhi_2019_Data.xlsx"), sheet = 1) %>%
  select("Station_code" = StationCode_2019, "CWV" = CWVDailyMean_2019, "ELV" = Elevation_2019, 
         "AOD" = Corrected_DailyMeanAOD_2019, "PM2.5" = Corrected_PM25DailyMean_2019,
         "Temp" = TempDailyMean_2019, "RH" = RHDailyMean_2019, "NDVI" = RHDailyMean_2019,
         "WD" = WDDailyMean_2019, "WS" = WSDailyMean_2019, "BLH" = BLHDailyMean_2019,
         "Press" = PressureDailyMean_2019, "season" = Season_2019, "day" = JulianDay_2019) %>%
  mutate_at(c("CWV", "ELV", "AOD", "PM2.5", "Temp", "RH", "NDVI", "WD", "WS", "BLH", "Press"), as.numeric) %>% 
  filter(!is.nan(PM2.5)) 


file_shared$Station_code <- as.factor(file_shared$Station_code)
file_shared$season <- as.factor(file_shared$season)
file_shared$day <- as.factor(file_shared$day)

# h2o.shutdown()
h2o.no_progress()
h2o.init(max_mem_size = "22g")

file_shared <- as.h2o(file_shared)

splits <- h2o.splitFrame(
  data = file_shared,
  ratios = c(0.7, 0),   
  destination_frames = c("train_hex", "valid_hex", "test_hex"), seed = 108
)
train <- splits[[1]]
valid <- splits[[2]]
test  <- splits[[3]]

response <- "PM2.5"
features <- setdiff(names(train), c(response))
h2o.describe(file_shared)

search_criteria <- list(strategy = "RandomDiscrete", 
                        stopping_metric = "mse",
                        stopping_tolerance = 1e-4,
                        max_runtime_secs = 60,
                        seed = 108)

# uploaded_model <- h2o.upload_model(my_local_model)

# collect the results and sort by our model performance metric of choice
hyper_grid <- list(
  ntrees      = seq(200, 400, by = 150),
  mtries      = seq(1, 6, by = 5),
  max_depth   = seq(10, 40, by = 20),
  min_rows    = seq(3, 6, by = 3),
  nbins       = seq(10, 40, by = 20),
  sample_rate = c(.55, .632, .75, .8)
)


# Find Parameters: Use Hyper Parameter Tuning on a "Training Dataset" that sections your training data into 5-Folds. The output at Stage 1 is the parameter set.
# build grid search 
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = features, 
  y = response, 
  training_frame = train,
  keep_cross_validation_predictions = TRUE,
  keep_cross_validation_models = TRUE,
  keep_cross_validation_fold_assignment = TRUE, 
  nfolds = 10,  
  categorical_encoding = "AUTO",
  hyper_params = hyper_grid,
  search_criteria = search_criteria
)

random_grid

# collect the results and sort by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "rf_grid", 
  sort_by = "mse", 
  decreasing = FALSE
)
print(grid_perf)


# Grab the model_id for the top model, chosen by validation error
best_model_id <- grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.varimp(best_model)
h2o.varimp_plot(best_model)
best_model

# Save model
model_path <- h2o.saveModel(object = best_model, path = getwd(), force = TRUE)
print(model_path)
saved_model <- h2o.loadModel(model_path)

h2o.scoreHistory(best_model)

# Get the CV models from the `best_model` object
cv_models <- sapply(best_model@model$cross_validation_models, 
                    function(i) h2o.getModel(i$name))
cv_models

# model_path <- h2o.saveModel(object = cv_models, path = getwd(), force = TRUE)
# print(model_path)

# Plot the scoring history over time
plot(cv_models[[1]], 
     timestep = "epochs", 
     metric = "rmse")

# Compare and Select Best Model: Evaluate the performance on a hidden "Test Dataset". The ouput at Stage 2 is that we determine best model.
# Now let's evaluate the model performance on a test set
best_model_perf <- h2o.performance(model = best_model, newdata = test)
best_model_perf
# RMSE of best model
h2o.mse(best_model_perf) %>% sqrt()


test$h2o_rf <- predict(best_model, test)
test <- as.data.frame(test)
write.csv(test, "test_h2o_RF.csv")

file_shared$h2o_rf <- predict(best_model, file_shared)

# Train Final Model: Once we have selected the best model, we train on the full dataset. This model goes into production.
model_drf <- h2o.randomForest(x = features, 
                             y = response, 
                             training_frame = file_shared,
                             ntrees = 100,
                             max_depth = 30,                             
                             nbins = 30, 
                             min_rows = 3,
                             mtries = 6,
                             sample_rate = 0.75,
                             keep_cross_validation_predictions = TRUE,
                             keep_cross_validation_models = TRUE,
                             keep_cross_validation_fold_assignment = TRUE, 
                             nfolds = 10)

model_drf
cvpreds_id <- model_drf@model$cross_validation_holdout_predictions_frame_id$name
file_shared$cvpreds <- h2o.getFrame(cvpreds_id)

h2o.varimp(model_drf)
h2o.varimp_plot(model_drf)
file_shared$h2o_rf_m <- predict(model_drf, file_shared)
file_shared <- as.data.frame(file_shared)
ggplot(file_shared, aes(PM2.5, h2o_rf_m)) + geom_point() + geom_smooth(method = "lm")
summary(lm(PM2.5 ~ h2o_rf_m, data = file_shared))
mean(abs((file_shared$PM2.5 - file_shared$h2o_rf_m) / file_shared$PM2.5), na.rm = TRUE) * 100

write.csv(file_shared, "h2o_RF.csv")


```

